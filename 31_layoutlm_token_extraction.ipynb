{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3Model\n",
    "import pandas as pd\n",
    "\n",
    "# Prevent tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Setup paths\n",
    "DEMO_PATH = \"demo_documents\"\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "model = LayoutLMv3Model.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "\n",
    "# Helper function to list documents\n",
    "def list_demo_documents():\n",
    "    \"\"\"List all PNG files in demo directory\"\"\"\n",
    "    if not os.path.exists(DEMO_PATH):\n",
    "        print(f\"Demo directory {DEMO_PATH} does not exist!\")\n",
    "        return []\n",
    "        \n",
    "    documents = [f for f in os.listdir(DEMO_PATH) if f.endswith('.png')]\n",
    "    print(\"\\nAvailable documents:\")\n",
    "    for idx, doc in enumerate(documents):\n",
    "        print(f\"{idx}: {doc}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available documents:\n",
      "0: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "1: 15031152_Topmech_320000220010442023_1.png\n",
      "2: 50001213_KSU_A-Technik_320000220006912023_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaclavpechtor/Code/home_projects/document-processing-demo/env-layoutlm/lib/python3.11/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing document: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "\n",
      "Token Information:\n",
      "  token  embedding_norm   position\n",
      "0     W       19.064436  (334, 59)\n",
      "1   urd       19.429543  (334, 59)\n",
      "2     e       19.717766  (334, 59)\n",
      "3   ver       18.909647  (425, 56)\n",
      "4    re       19.791399  (425, 56)\n",
      "5   chn       19.387173  (425, 56)\n",
      "6    et       19.013855  (425, 56)\n",
      "7    an       19.598726  (571, 58)\n",
      "8   Pin       20.020063  (337, 85)\n",
      "9     :       20.155340  (337, 85)\n"
     ]
    }
   ],
   "source": [
    "def process_document_tokens(image_path, processor, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Process document and get token embeddings and their positions\n",
    "    \"\"\"\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    encoding = processor(\n",
    "        image,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Move to device and get model outputs\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    outputs = model(**encoding)\n",
    "    \n",
    "    # Get tokens and their embeddings\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    embeddings = outputs.last_hidden_state[0]  # Remove batch dimension\n",
    "    boxes = encoding['bbox'][0]\n",
    "    \n",
    "    # Create a list of token information\n",
    "    token_info = []\n",
    "    for token, embedding, box in zip(tokens, embeddings, boxes):\n",
    "        # Skip special tokens\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "            \n",
    "        # Clean up token (remove tokenizer artifacts)\n",
    "        clean_token = token.replace('Ġ', '')\n",
    "        \n",
    "        token_info.append({\n",
    "            'token': clean_token,\n",
    "            'embedding': embedding.detach().cpu(),  # Move back to CPU for inspection\n",
    "            'box': box.cpu().tolist(),\n",
    "            'embedding_norm': torch.norm(embedding).item()  # Magnitude of embedding\n",
    "        })\n",
    "    \n",
    "    return token_info\n",
    "\n",
    "# Example usage\n",
    "def analyze_token_embeddings(image_path):\n",
    "    \"\"\"\n",
    "    Analyze token embeddings of a document\n",
    "    \"\"\"\n",
    "    # Process document\n",
    "    token_info = process_document_tokens(image_path, processor, model)\n",
    "    \n",
    "    # Create DataFrame for easy viewing\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'token': info['token'],\n",
    "            'embedding_norm': info['embedding_norm'],\n",
    "            'position': f\"({info['box'][0]}, {info['box'][1]})\",\n",
    "        }\n",
    "        for info in token_info\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nAnalyzing document: {os.path.basename(image_path)}\")\n",
    "    print(\"\\nToken Information:\")\n",
    "    print(df.head(10))  # Show first 10 tokens\n",
    "    \n",
    "    return token_info, df\n",
    "\n",
    "# Try it on a document\n",
    "documents = list_demo_documents()\n",
    "if documents:\n",
    "    image_path = os.path.join(DEMO_PATH, documents[0])\n",
    "    token_info, df = analyze_token_embeddings(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaclavpechtor/Code/home_projects/document-processing-demo/env-layoutlm/lib/python3.11/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing document: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "\n",
      "Reconstructed Words:\n",
      "                   word              position  avg_embedding_norm\n",
      "0     Wurdeverrechnetan    [334, 59, 416, 74]           19.364068\n",
      "1                  Pin:   [337, 85, 383, 100]           20.087702\n",
      "2               22-134.    [396, 79, 444, 95]           19.596047\n",
      "3             506patum:    [454, 72, 583, 95]           19.137784\n",
      "4                C4-01.  [337, 109, 441, 130]           19.552668\n",
      "5                   CS/  [448, 109, 472, 130]           19.571903\n",
      "6                Visum:  [475, 114, 527, 129]           19.725094\n",
      "7                 burwi  [531, 105, 570, 127]           17.644155\n",
      "8                Kanton  [105, 142, 166, 151]           19.870708\n",
      "9               Ztirich  [172, 141, 226, 151]           20.517123\n",
      "10  Strassenverkehrsamt  [105, 162, 287, 172]           20.248379\n",
      "11                 Frau  [105, 183, 143, 193]           20.654235\n",
      "12               Sandra  [149, 182, 209, 193]           19.589653\n",
      "13              MÃ©ckli  [215, 182, 270, 193]           20.946383\n",
      "14            Zulassung  [105, 204, 191, 217]           20.623022\n",
      "\n",
      "Tokens with highest embedding norms (might be important):\n",
      "Token: sh, Norm: 22.03, Position: [807, 111, 912, 120]\n",
      "Token: iva, Norm: 21.89, Position: [807, 111, 912, 120]\n",
      "Token: Shiva, Norm: 21.72, Position: [758, 47, 816, 59]\n",
      "Token: @, Norm: 21.63, Position: [807, 111, 912, 120]\n",
      "Token: 400, Norm: 21.61, Position: [850, 73, 881, 81]\n",
      "\n",
      "Potential numeric values:\n",
      "Value: 22, Position: [396, 79, 444, 95]\n",
      "Value: 134, Position: [454, 72, 583, 95]\n",
      "Value: 506, Position: [454, 72, 583, 95]\n",
      "Value: 4, Position: [337, 109, 441, 130]\n",
      "Value: 01, Position: [337, 109, 441, 130]\n"
     ]
    }
   ],
   "source": [
    "def analyze_token_embeddings(image_path):\n",
    "    \"\"\"\n",
    "    Enhanced analysis of token embeddings\n",
    "    \"\"\"\n",
    "    token_info = process_document_tokens(image_path, processor, model)\n",
    "    \n",
    "    # Reconstruct words and their positions\n",
    "    current_word = []\n",
    "    current_position = None\n",
    "    words = []\n",
    "    \n",
    "    for info in token_info:\n",
    "        token = info['token']\n",
    "        position = info['box']\n",
    "        \n",
    "        # New word starts with capital letter or after punctuation\n",
    "        if (token[0].isupper() and current_word) or \\\n",
    "           (current_word and current_word[-1]['token'] in '.,;:!?'):\n",
    "            # Save current word\n",
    "            words.append({\n",
    "                'word': ''.join(t['token'] for t in current_word),\n",
    "                'position': current_position,\n",
    "                'avg_embedding_norm': sum(t['embedding_norm'] for t in current_word) / len(current_word)\n",
    "            })\n",
    "            current_word = []\n",
    "            current_position = None\n",
    "        \n",
    "        current_word.append(info)\n",
    "        if not current_position:\n",
    "            current_position = position\n",
    "    \n",
    "    # Add last word\n",
    "    if current_word:\n",
    "        words.append({\n",
    "            'word': ''.join(t['token'] for t in current_word),\n",
    "            'position': current_position,\n",
    "            'avg_embedding_norm': sum(t['embedding_norm'] for t in current_word) / len(current_word)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(words)\n",
    "    \n",
    "    print(f\"\\nAnalyzing document: {os.path.basename(image_path)}\")\n",
    "    print(\"\\nReconstructed Words:\")\n",
    "    print(df.head(15))\n",
    "    \n",
    "    # Find interesting patterns\n",
    "    print(\"\\nTokens with highest embedding norms (might be important):\")\n",
    "    high_norm = sorted(token_info, key=lambda x: x['embedding_norm'], reverse=True)[:5]\n",
    "    for t in high_norm:\n",
    "        print(f\"Token: {t['token']}, Norm: {t['embedding_norm']:.2f}, Position: {t['box']}\")\n",
    "        \n",
    "    # Look for number patterns\n",
    "    print(\"\\nPotential numeric values:\")\n",
    "    numbers = [t for t in token_info if any(c.isdigit() for c in t['token'])]\n",
    "    for n in numbers[:5]:  # Show first 5\n",
    "        print(f\"Value: {n['token']}, Position: {n['box']}\")\n",
    "    \n",
    "    return words, df\n",
    "\n",
    "# Try the enhanced analysis\n",
    "words, df = analyze_token_embeddings(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaclavpechtor/Code/home_projects/document-processing-demo/env-layoutlm/lib/python3.11/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing document: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "\n",
      "Aligned Text Blocks:\n",
      "\n",
      "At x=105:\n",
      "K anton\n",
      "Str ass en ver ke h rs am t\n",
      "Fra u\n",
      "Z ul ass ung\n",
      "8 408\n",
      "Bad en ,\n",
      "Le ist ung s dat um\n",
      ":\n",
      "An sp re ch sp art ner\n",
      ":\n",
      "Total\n",
      "\n",
      "At x=106:\n",
      "PIN\n",
      "Land\n",
      ":\n",
      "U bers etz ung\n",
      "Sp es en\n",
      "Bank verb ind ung\n",
      ":\n",
      "\n",
      "At x=142:\n",
      "33 .. 7 31 .\n",
      "9 06\n",
      "\n",
      "At x=207:\n",
      "25 .\n",
      "02 .\n",
      "20 23\n",
      "\n",
      "At x=254:\n",
      "03 .\n",
      "01 .\n",
      "20 23\n",
      "\n",
      "At x=285:\n",
      "IB\n",
      "AN\n",
      "\n",
      "At x=334:\n",
      "W urd e\n",
      "CH 26\n",
      "\n",
      "At x=337:\n",
      "Pin\n",
      ":\n",
      "pat um\n",
      ":\n",
      "C 4\n",
      "- 01 .\n",
      "\n",
      "At x=343:\n",
      "490 .\n",
      "00\n",
      "\n",
      "At x=396:\n",
      "22\n",
      "-\n",
      "\n",
      "At x=454:\n",
      "134 .\n",
      "506\n",
      "\n",
      "At x=475:\n",
      "/\n",
      "Vis um\n",
      ":\n",
      "\n",
      "At x=758:\n",
      "Shiva\n",
      "F\n",
      "Ã© h ren we g\n",
      "Tele f on\n",
      ":\n",
      "Mobile\n",
      ":\n",
      "\n",
      "At x=759:\n",
      "E\n",
      "-\n",
      "Mail\n",
      ":\n",
      "\n",
      "At x=760:\n",
      "Fr .\n",
      "Fr .\n",
      "\n",
      "At x=788:\n",
      "180 .\n",
      "00\n",
      "00 .\n",
      "00\n",
      "180 .\n",
      "00\n",
      "\n",
      "At x=807:\n",
      "sh iva\n",
      "@ sie gen .\n",
      "ch\n",
      "\n",
      "Potential Amounts:\n",
      "Empty DataFrame\n",
      "Columns: [word, position]\n",
      "Index: []\n",
      "\n",
      "High Importance Words:\n",
      "       word  avg_embedding_norm              position\n",
      "99   sh iva           21.960420  [807, 111, 912, 120]\n",
      "77    Shiva           21.719246    [758, 47, 816, 59]\n",
      "82    5 400           21.597796    [850, 73, 881, 81]\n",
      "101      ch           21.452927  [807, 111, 912, 120]\n",
      "94       40           21.368423   [886, 98, 902, 106]\n"
     ]
    }
   ],
   "source": [
    "def analyze_token_embeddings(image_path):\n",
    "    \"\"\"\n",
    "    Further enhanced analysis with better word separation and pattern recognition\n",
    "    \"\"\"\n",
    "    token_info = process_document_tokens(image_path, processor, model)\n",
    "    \n",
    "    # Improved word reconstruction\n",
    "    words = []\n",
    "    current_word = []\n",
    "    current_position = None\n",
    "    \n",
    "    for info in token_info:\n",
    "        token = info['token']\n",
    "        position = info['box']\n",
    "        \n",
    "        # Better word separation conditions\n",
    "        new_word = (\n",
    "            (token[0].isupper() and current_word) or  # Capital letter\n",
    "            (current_word and current_word[-1]['token'] in '.,;:!?') or  # Punctuation\n",
    "            (position != current_position) or  # Position change\n",
    "            (token in [':', '@', '/', '-'])  # Special characters\n",
    "        )\n",
    "        \n",
    "        if new_word and current_word:\n",
    "            words.append({\n",
    "                'word': ' '.join(t['token'] for t in current_word),\n",
    "                'position': current_position,\n",
    "                'avg_embedding_norm': sum(t['embedding_norm'] for t in current_word) / len(current_word),\n",
    "                'x': current_position[0],  # Add x coordinate for alignment analysis\n",
    "                'y': current_position[1]   # Add y coordinate for vertical position\n",
    "            })\n",
    "            current_word = []\n",
    "        \n",
    "        current_word.append(info)\n",
    "        current_position = position\n",
    "    \n",
    "    # Add last word\n",
    "    if current_word:\n",
    "        words.append({\n",
    "            'word': ' '.join(t['token'] for t in current_word),\n",
    "            'position': current_position,\n",
    "            'avg_embedding_norm': sum(t['embedding_norm'] for t in current_word) / len(current_word),\n",
    "            'x': current_position[0],\n",
    "            'y': current_position[1]\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(words)\n",
    "    \n",
    "    print(f\"\\nAnalyzing document: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Find aligned text blocks (same x coordinate)\n",
    "    print(\"\\nAligned Text Blocks:\")\n",
    "    x_positions = df.groupby('x')['word'].apply(list)\n",
    "    for x, words_at_x in x_positions.items():\n",
    "        if len(words_at_x) > 1:  # Show only if multiple words aligned\n",
    "            print(f\"\\nAt x={x}:\")\n",
    "            print('\\n'.join(words_at_x))\n",
    "    \n",
    "    # Look for amount patterns\n",
    "    print(\"\\nPotential Amounts:\")\n",
    "    amount_pattern = r'\\d+[\\.,]\\d{2}'\n",
    "    amounts = df[df['word'].str.contains(amount_pattern, na=False)]\n",
    "    print(amounts[['word', 'position']].to_string())\n",
    "    \n",
    "    # Find high-importance tokens\n",
    "    print(\"\\nHigh Importance Words:\")\n",
    "    important = df.nlargest(5, 'avg_embedding_norm')\n",
    "    print(important[['word', 'avg_embedding_norm', 'position']].to_string())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Try the enhanced analysis\n",
    "df = analyze_token_embeddings(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available documents:\n",
      "0: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "1: 15031152_Topmech_320000220010442023_1.png\n",
      "2: 50001213_KSU_A-Technik_320000220006912023_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaclavpechtor/Code/home_projects/document-processing-demo/env-layoutlm/lib/python3.11/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing document: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "\n",
      "Semantic Groups Found:\n",
      "\n",
      "Group 0:\n",
      "Text: W urd e\n",
      "Position: (334, 59)\n",
      "Importance (avg norm): 19.40\n",
      "→ Potential header information\n",
      "\n",
      "Group 1:\n",
      "Text: ver re chn et\n",
      "Position: (425, 56)\n",
      "Importance (avg norm): 19.28\n",
      "→ Potential header information\n",
      "\n",
      "Group 2:\n",
      "Text: Pin :\n",
      "Position: (337, 85)\n",
      "Importance (avg norm): 20.09\n",
      "→ Potential header information\n",
      "\n",
      "Group 3:\n",
      "Text: 22 -\n",
      "Position: (396, 79)\n",
      "Importance (avg norm): 19.57\n",
      "→ Potential header information\n",
      "\n",
      "Group 4:\n",
      "Text: 134 . 506\n",
      "Position: (454, 72)\n",
      "Importance (avg norm): 19.60\n",
      "→ Potential header information\n",
      "\n",
      "Group 5:\n",
      "Text: pat um : C 4 - 01 .\n",
      "Position: (337, 109)\n",
      "Importance (avg norm): 19.34\n",
      "\n",
      "Group 6:\n",
      "Text: / Vis um :\n",
      "Position: (475, 114)\n",
      "Importance (avg norm): 19.82\n",
      "\n",
      "Group 7:\n",
      "Text: K anton\n",
      "Position: (105, 142)\n",
      "Importance (avg norm): 19.87\n",
      "\n",
      "Group 8:\n",
      "Text: Z t ir ich Str ass en ver ke h rs am t Sandra\n",
      "Position: (127, 157)\n",
      "Importance (avg norm): 20.28\n",
      "→ Potential address block\n",
      "\n",
      "Group 9:\n",
      "Text: Fra u\n",
      "Position: (105, 183)\n",
      "Importance (avg norm): 20.65\n",
      "\n",
      "Group 10:\n",
      "Text: MÃ© ck li Winter thur\n",
      "Position: (207, 190)\n",
      "Importance (avg norm): 20.84\n",
      "\n",
      "Group 11:\n",
      "Text: Z ul ass ung\n",
      "Position: (105, 204)\n",
      "Importance (avg norm): 20.62\n",
      "\n",
      "Group 12:\n",
      "Text: Tag gen berg str asse Winter thur\n",
      "Position: (118, 230)\n",
      "Importance (avg norm): 20.67\n",
      "→ Potential address block\n",
      "\n",
      "Group 13:\n",
      "Text: 8 408\n",
      "Position: (105, 246)\n",
      "Importance (avg norm): 20.57\n",
      "\n",
      "Group 14:\n",
      "Text: Bad en ,\n",
      "Position: (105, 353)\n",
      "Importance (avg norm): 20.71\n",
      "\n",
      "Group 15:\n",
      "Text: 3 .\n",
      "Position: (169, 353)\n",
      "Importance (avg norm): 20.81\n",
      "\n",
      "Group 16:\n",
      "Text: Jan u ar\n",
      "Position: (188, 353)\n",
      "Importance (avg norm): 20.65\n",
      "\n",
      "Group 17:\n",
      "Text: 20 23\n",
      "Position: (249, 353)\n",
      "Importance (avg norm): 20.75\n",
      "\n",
      "Group 18:\n",
      "Text: Re chn ung\n",
      "Position: (437, 393)\n",
      "Importance (avg norm): 20.59\n",
      "\n",
      "Group 19:\n",
      "Text: PIN Land :\n",
      "Position: (106, 433)\n",
      "Importance (avg norm): 20.55\n",
      "\n",
      "Group 20:\n",
      "Text: 33 .. 7 31 . 9 06 Iran\n",
      "Position: (144, 419)\n",
      "Importance (avg norm): 20.37\n",
      "→ Potential address block\n",
      "\n",
      "Group 21:\n",
      "Text: Le ist ung s dat um : An sp re ch sp art ner :\n",
      "Position: (105, 504)\n",
      "Importance (avg norm): 20.54\n",
      "→ Potential address block\n",
      "\n",
      "Group 22:\n",
      "Text: 03 . 01 . 20 23 Shiva\n",
      "Position: (257, 494)\n",
      "Importance (avg norm): 20.55\n",
      "\n",
      "Group 23:\n",
      "Text: Sie gen\n",
      "Position: (322, 515)\n",
      "Importance (avg norm): 20.76\n",
      "\n",
      "Group 24:\n",
      "Text: U bers etz ung\n",
      "Position: (106, 565)\n",
      "Importance (avg norm): 20.24\n",
      "\n",
      "Group 25:\n",
      "Text: Se it en\n",
      "Position: (235, 566)\n",
      "Importance (avg norm): 20.46\n",
      "\n",
      "Group 26:\n",
      "Text: 490 . 00\n",
      "Position: (343, 565)\n",
      "Importance (avg norm): 20.25\n",
      "\n",
      "Group 27:\n",
      "Text: Sp es en Total\n",
      "Position: (106, 598)\n",
      "Importance (avg norm): 20.35\n",
      "\n",
      "Group 28:\n",
      "Text: Bank verb ind ung :\n",
      "Position: (106, 666)\n",
      "Importance (avg norm): 20.17\n",
      "\n",
      "Group 29:\n",
      "Text: A arg au ische\n",
      "Position: (288, 666)\n",
      "Importance (avg norm): 20.61\n",
      "\n",
      "Group 30:\n",
      "Text: Kant onal bank\n",
      "Position: (399, 665)\n",
      "Importance (avg norm): 20.21\n",
      "\n",
      "Group 31:\n",
      "Text: A ar au , IB AN\n",
      "Position: (284, 700)\n",
      "Importance (avg norm): 20.62\n",
      "\n",
      "Group 32:\n",
      "Text: CH 26\n",
      "Position: (334, 716)\n",
      "Importance (avg norm): 20.35\n",
      "\n",
      "Group 33:\n",
      "Text: 00 76\n",
      "Position: (386, 716)\n",
      "Importance (avg norm): 20.66\n",
      "\n",
      "Group 34:\n",
      "Text: 101 6\n",
      "Position: (435, 716)\n",
      "Importance (avg norm): 20.85\n",
      "\n",
      "Group 35:\n",
      "Text: 11 84\n",
      "Position: (484, 716)\n",
      "Importance (avg norm): 20.59\n",
      "\n",
      "Group 36:\n",
      "Text: 4 349\n",
      "Position: (533, 715)\n",
      "Importance (avg norm): 20.34\n",
      "\n",
      "Group 37:\n",
      "Text: Z ahl bar\n",
      "Position: (107, 818)\n",
      "Importance (avg norm): 20.45\n",
      "\n",
      "Group 38:\n",
      "Text: 25 . 02 . 20 23\n",
      "Position: (207, 817)\n",
      "Importance (avg norm): 20.59\n",
      "\n",
      "Group 39:\n",
      "Text: Shiva F Ã© h ren we g Tele f on : Mobile : E - Mail :\n",
      "Position: (758, 86)\n",
      "Importance (avg norm): 20.81\n",
      "→ Potential header information\n",
      "\n",
      "Group 40:\n",
      "Text: Sie gen 6 , 5 400 Bad en 0 56 222 11 70 0 79 257 20 40 sh iva @ sie gen . ch\n",
      "Position: (835, 89)\n",
      "Importance (avg norm): 21.24\n",
      "→ Potential header information\n",
      "\n",
      "Group 41:\n",
      "Text: Fr . Fr . Fr .\n",
      "Position: (760, 589)\n",
      "Importance (avg norm): 20.82\n",
      "\n",
      "Group 42:\n",
      "Text: 180 . 00 00 . 00 180 . 00\n",
      "Position: (788, 589)\n",
      "Importance (avg norm): 20.58\n",
      "\n",
      "Strongly Related Terms:\n",
      "W ↔ urd (similarity: 0.85)\n",
      "W ↔ e (similarity: 0.82)\n",
      "urd ↔ e (similarity: 0.82)\n",
      "urd ↔ chn (similarity: 0.80)\n",
      "urd ↔ et (similarity: 0.81)\n",
      "e ↔ re (similarity: 0.81)\n",
      "e ↔ et (similarity: 0.87)\n",
      "ver ↔ re (similarity: 0.88)\n",
      "ver ↔ chn (similarity: 0.84)\n",
      "ver ↔ et (similarity: 0.83)\n",
      "re ↔ chn (similarity: 0.86)\n",
      "re ↔ et (similarity: 0.85)\n",
      "chn ↔ et (similarity: 0.87)\n",
      ": ↔ - (similarity: 0.81)\n",
      ": ↔ : (similarity: 0.95)\n",
      ": ↔ : (similarity: 0.92)\n",
      "22 ↔ C (similarity: 0.80)\n",
      "- ↔ . (similarity: 0.85)\n",
      "- ↔ - (similarity: 0.95)\n",
      "- ↔ . (similarity: 0.86)\n",
      "134 ↔ 506 (similarity: 0.82)\n",
      ". ↔ - (similarity: 0.83)\n",
      ". ↔ . (similarity: 0.88)\n",
      "pat ↔ Vis (similarity: 0.84)\n",
      "pat ↔ bur (similarity: 0.83)\n",
      "pat ↔ Str (similarity: 0.81)\n",
      "um ↔ um (similarity: 0.93)\n",
      ": ↔ - (similarity: 0.80)\n",
      ": ↔ : (similarity: 0.93)\n",
      "C ↔ 4 (similarity: 0.83)\n",
      "C ↔ - (similarity: 0.84)\n",
      "C ↔ 01 (similarity: 0.84)\n",
      "C ↔ CS (similarity: 0.83)\n",
      "4 ↔ - (similarity: 0.81)\n",
      "4 ↔ 01 (similarity: 0.83)\n",
      "- ↔ 01 (similarity: 0.84)\n",
      "- ↔ . (similarity: 0.88)\n",
      "01 ↔ . (similarity: 0.80)\n",
      "01 ↔ CS (similarity: 0.81)\n",
      "CS ↔ / (similarity: 0.82)\n",
      "CS ↔ bur (similarity: 0.82)\n",
      "CS ↔ wi (similarity: 0.83)\n",
      "Vis ↔ um (similarity: 0.81)\n",
      "bur ↔ wi (similarity: 0.90)\n",
      "K ↔ anton (similarity: 0.83)\n",
      "K ↔ Z (similarity: 0.85)\n",
      "K ↔ Str (similarity: 0.83)\n",
      "K ↔ en (similarity: 0.81)\n",
      "K ↔ ver (similarity: 0.81)\n",
      "K ↔ ke (similarity: 0.83)\n",
      "anton ↔ Z (similarity: 0.81)\n",
      "anton ↔ ich (similarity: 0.83)\n",
      "anton ↔ Str (similarity: 0.81)\n",
      "anton ↔ en (similarity: 0.80)\n",
      "anton ↔ ver (similarity: 0.82)\n",
      "anton ↔ Sandra (similarity: 0.83)\n",
      "anton ↔ ung (similarity: 0.80)\n",
      "Z ↔ t (similarity: 0.84)\n",
      "Z ↔ ir (similarity: 0.84)\n",
      "Z ↔ ich (similarity: 0.86)\n",
      "Z ↔ Str (similarity: 0.82)\n",
      "Z ↔ ver (similarity: 0.82)\n",
      "Z ↔ MÃ© (similarity: 0.80)\n",
      "Z ↔ Z (similarity: 0.88)\n",
      "t ↔ ir (similarity: 0.85)\n",
      "t ↔ ich (similarity: 0.81)\n",
      "t ↔ t (similarity: 0.85)\n",
      "ir ↔ ich (similarity: 0.86)\n",
      "ich ↔ ke (similarity: 0.80)\n",
      "ich ↔ t (similarity: 0.81)\n",
      "ich ↔ li (similarity: 0.81)\n",
      "ich ↔ ung (similarity: 0.82)\n",
      "Str ↔ ver (similarity: 0.87)\n",
      "Str ↔ ke (similarity: 0.81)\n",
      "Str ↔ rs (similarity: 0.84)\n",
      "Str ↔ Fra (similarity: 0.82)\n",
      "Str ↔ Sandra (similarity: 0.80)\n",
      "Str ↔ Z (similarity: 0.81)\n",
      "Str ↔ ul (similarity: 0.81)\n",
      "Str ↔ Winter (similarity: 0.81)\n",
      "Str ↔ Tag (similarity: 0.86)\n",
      "Str ↔ str (similarity: 0.85)\n",
      "ass ↔ en (similarity: 0.85)\n",
      "ass ↔ ver (similarity: 0.82)\n",
      "ass ↔ ke (similarity: 0.85)\n",
      "ass ↔ h (similarity: 0.83)\n",
      "ass ↔ rs (similarity: 0.85)\n",
      "ass ↔ am (similarity: 0.85)\n",
      "ass ↔ ul (similarity: 0.80)\n",
      "ass ↔ ass (similarity: 0.93)\n",
      "ass ↔ gen (similarity: 0.80)\n",
      "ass ↔ asse (similarity: 0.83)\n",
      "en ↔ ver (similarity: 0.87)\n",
      "en ↔ ke (similarity: 0.86)\n",
      "en ↔ h (similarity: 0.86)\n",
      "en ↔ rs (similarity: 0.83)\n",
      "en ↔ am (similarity: 0.86)\n",
      "en ↔ t (similarity: 0.86)\n",
      "en ↔ ass (similarity: 0.81)\n",
      "en ↔ ung (similarity: 0.83)\n",
      "en ↔ gen (similarity: 0.84)\n",
      "en ↔ berg (similarity: 0.83)\n",
      "ver ↔ ke (similarity: 0.85)\n",
      "ver ↔ h (similarity: 0.85)\n",
      "ver ↔ rs (similarity: 0.87)\n",
      "ver ↔ am (similarity: 0.85)\n",
      "ver ↔ t (similarity: 0.81)\n",
      "ver ↔ Fra (similarity: 0.80)\n",
      "ver ↔ Z (similarity: 0.81)\n",
      "ver ↔ Winter (similarity: 0.80)\n",
      "ver ↔ Tag (similarity: 0.82)\n",
      "ver ↔ berg (similarity: 0.84)\n",
      "ver ↔ str (similarity: 0.84)\n",
      "ke ↔ h (similarity: 0.87)\n",
      "ke ↔ rs (similarity: 0.86)\n",
      "ke ↔ am (similarity: 0.84)\n",
      "ke ↔ t (similarity: 0.83)\n",
      "ke ↔ ul (similarity: 0.82)\n",
      "h ↔ rs (similarity: 0.88)\n",
      "h ↔ am (similarity: 0.88)\n",
      "h ↔ t (similarity: 0.87)\n",
      "h ↔ ul (similarity: 0.83)\n",
      "h ↔ ass (similarity: 0.82)\n",
      "h ↔ ung (similarity: 0.82)\n",
      "h ↔ berg (similarity: 0.83)\n",
      "h ↔ str (similarity: 0.81)\n",
      "h ↔ asse (similarity: 0.80)\n",
      "rs ↔ am (similarity: 0.87)\n",
      "rs ↔ t (similarity: 0.85)\n",
      "rs ↔ ul (similarity: 0.82)\n",
      "rs ↔ ass (similarity: 0.81)\n",
      "rs ↔ berg (similarity: 0.82)\n",
      "rs ↔ str (similarity: 0.85)\n",
      "rs ↔ asse (similarity: 0.81)\n",
      "am ↔ t (similarity: 0.87)\n",
      "am ↔ ass (similarity: 0.83)\n",
      "am ↔ ung (similarity: 0.84)\n",
      "am ↔ gen (similarity: 0.80)\n",
      "am ↔ berg (similarity: 0.82)\n",
      "am ↔ str (similarity: 0.81)\n",
      "am ↔ asse (similarity: 0.83)\n",
      "t ↔ ung (similarity: 0.84)\n",
      "t ↔ berg (similarity: 0.80)\n",
      "t ↔ asse (similarity: 0.81)\n",
      "Fra ↔ u (similarity: 0.80)\n",
      "Fra ↔ Sandra (similarity: 0.82)\n",
      "u ↔ ul (similarity: 0.83)\n",
      "u ↔ ung (similarity: 0.80)\n",
      "MÃ© ↔ ck (similarity: 0.81)\n",
      "Z ↔ ul (similarity: 0.83)\n",
      "Z ↔ ass (similarity: 0.80)\n",
      "Z ↔ ung (similarity: 0.84)\n",
      "Z ↔ Tag (similarity: 0.82)\n",
      "Z ↔ str (similarity: 0.80)\n",
      "Z ↔ Z (similarity: 0.82)\n",
      "ul ↔ ass (similarity: 0.83)\n",
      "ul ↔ ung (similarity: 0.83)\n",
      "ass ↔ ung (similarity: 0.84)\n",
      "ass ↔ asse (similarity: 0.83)\n",
      "ung ↔ gen (similarity: 0.80)\n",
      "ung ↔ berg (similarity: 0.83)\n",
      "ung ↔ asse (similarity: 0.84)\n",
      "Winter ↔ thur (similarity: 0.80)\n",
      "Winter ↔ Winter (similarity: 0.95)\n",
      "thur ↔ thur (similarity: 0.95)\n",
      "Tag ↔ gen (similarity: 0.85)\n",
      "Tag ↔ berg (similarity: 0.86)\n",
      "Tag ↔ str (similarity: 0.83)\n",
      "Tag ↔ asse (similarity: 0.82)\n",
      "gen ↔ berg (similarity: 0.91)\n",
      "gen ↔ str (similarity: 0.83)\n",
      "gen ↔ asse (similarity: 0.83)\n",
      "berg ↔ str (similarity: 0.90)\n",
      "berg ↔ asse (similarity: 0.87)\n",
      "str ↔ asse (similarity: 0.89)\n",
      "Winter ↔ thur (similarity: 0.82)\n",
      "Bad ↔ Bad (similarity: 0.86)\n",
      "en ↔ en (similarity: 0.84)\n",
      ", ↔ . (similarity: 0.85)\n",
      ", ↔ : (similarity: 0.82)\n",
      ", ↔ , (similarity: 0.81)\n",
      ". ↔ . (similarity: 0.85)\n",
      "Jan ↔ ar (similarity: 0.81)\n",
      "u ↔ ar (similarity: 0.82)\n",
      "20 ↔ 20 (similarity: 0.88)\n",
      "23 ↔ 23 (similarity: 0.90)\n",
      "23 ↔ 23 (similarity: 0.82)\n",
      "Re ↔ chn (similarity: 0.82)\n",
      "chn ↔ ung (similarity: 0.83)\n",
      "33 ↔ .. (similarity: 0.81)\n",
      "33 ↔ 03 (similarity: 0.81)\n",
      ". ↔ . (similarity: 0.86)\n",
      ". ↔ . (similarity: 0.87)\n",
      "Land ↔ Iran (similarity: 0.81)\n",
      ": ↔ : (similarity: 0.92)\n",
      ": ↔ : (similarity: 0.92)\n",
      ": ↔ : (similarity: 0.83)\n",
      "Le ↔ ist (similarity: 0.82)\n",
      "Le ↔ ung (similarity: 0.81)\n",
      "Le ↔ An (similarity: 0.81)\n",
      "Le ↔ U (similarity: 0.85)\n",
      "Le ↔ etz (similarity: 0.82)\n",
      "Le ↔ Se (similarity: 0.81)\n",
      "ist ↔ ung (similarity: 0.80)\n",
      "ist ↔ dat (similarity: 0.83)\n",
      "ist ↔ um (similarity: 0.80)\n",
      "ist ↔ art (similarity: 0.81)\n",
      "ist ↔ bers (similarity: 0.81)\n",
      "ist ↔ etz (similarity: 0.83)\n",
      "ung ↔ s (similarity: 0.83)\n",
      "ung ↔ um (similarity: 0.85)\n",
      "ung ↔ ch (similarity: 0.82)\n",
      "ung ↔ etz (similarity: 0.83)\n",
      "ung ↔ ung (similarity: 0.90)\n",
      "ung ↔ ung (similarity: 0.83)\n",
      "s ↔ dat (similarity: 0.80)\n",
      "s ↔ um (similarity: 0.83)\n",
      "s ↔ ch (similarity: 0.81)\n",
      "s ↔ sp (similarity: 0.81)\n",
      "s ↔ ung (similarity: 0.81)\n",
      "dat ↔ um (similarity: 0.84)\n",
      "dat ↔ sp (similarity: 0.80)\n",
      "dat ↔ art (similarity: 0.80)\n",
      "dat ↔ etz (similarity: 0.80)\n",
      "um ↔ : (similarity: 0.81)\n",
      "um ↔ ner (similarity: 0.84)\n",
      "um ↔ etz (similarity: 0.80)\n",
      "um ↔ ung (similarity: 0.85)\n",
      "um ↔ en (similarity: 0.81)\n",
      ": ↔ : (similarity: 0.94)\n",
      ": ↔ ung (similarity: 0.82)\n",
      ": ↔ : (similarity: 0.86)\n",
      ". ↔ . (similarity: 0.94)\n",
      ". ↔ . (similarity: 0.88)\n",
      ". ↔ . (similarity: 0.81)\n",
      ". ↔ . (similarity: 0.86)\n",
      ". ↔ . (similarity: 0.80)\n",
      "20 ↔ 20 (similarity: 0.83)\n",
      "23 ↔ 23 (similarity: 0.83)\n",
      "An ↔ sp (similarity: 0.86)\n",
      "An ↔ re (similarity: 0.84)\n",
      "An ↔ ch (similarity: 0.84)\n",
      "An ↔ sp (similarity: 0.85)\n",
      "An ↔ art (similarity: 0.82)\n",
      "An ↔ ner (similarity: 0.81)\n",
      "An ↔ U (similarity: 0.82)\n",
      "An ↔ Sp (similarity: 0.84)\n",
      "An ↔ en (similarity: 0.80)\n",
      "sp ↔ re (similarity: 0.88)\n",
      "sp ↔ ch (similarity: 0.87)\n",
      "sp ↔ sp (similarity: 0.96)\n",
      "sp ↔ art (similarity: 0.88)\n",
      "sp ↔ ner (similarity: 0.85)\n",
      "sp ↔ bers (similarity: 0.84)\n",
      "sp ↔ etz (similarity: 0.81)\n",
      "sp ↔ Sp (similarity: 0.86)\n",
      "sp ↔ es (similarity: 0.81)\n",
      "sp ↔ en (similarity: 0.80)\n",
      "re ↔ ch (similarity: 0.89)\n",
      "re ↔ sp (similarity: 0.88)\n",
      "re ↔ art (similarity: 0.87)\n",
      "re ↔ ner (similarity: 0.85)\n",
      "re ↔ bers (similarity: 0.82)\n",
      "re ↔ etz (similarity: 0.81)\n",
      "re ↔ en (similarity: 0.82)\n",
      "ch ↔ sp (similarity: 0.88)\n",
      "ch ↔ art (similarity: 0.84)\n",
      "ch ↔ ner (similarity: 0.85)\n",
      "ch ↔ etz (similarity: 0.82)\n",
      "ch ↔ ung (similarity: 0.80)\n",
      "ch ↔ en (similarity: 0.80)\n",
      "ch ↔ ind (similarity: 0.80)\n",
      "sp ↔ art (similarity: 0.90)\n",
      "sp ↔ ner (similarity: 0.87)\n",
      "sp ↔ bers (similarity: 0.82)\n",
      "sp ↔ etz (similarity: 0.81)\n",
      "sp ↔ Sp (similarity: 0.86)\n",
      "art ↔ ner (similarity: 0.90)\n",
      "art ↔ bers (similarity: 0.80)\n",
      "art ↔ etz (similarity: 0.82)\n",
      "ner ↔ etz (similarity: 0.82)\n",
      "ner ↔ ung (similarity: 0.83)\n",
      "ner ↔ en (similarity: 0.84)\n",
      "ner ↔ ung (similarity: 0.80)\n",
      ": ↔ ung (similarity: 0.80)\n",
      ": ↔ : (similarity: 0.91)\n",
      "U ↔ bers (similarity: 0.83)\n",
      "U ↔ etz (similarity: 0.83)\n",
      "U ↔ ung (similarity: 0.82)\n",
      "U ↔ Sp (similarity: 0.82)\n",
      "bers ↔ etz (similarity: 0.87)\n",
      "bers ↔ ung (similarity: 0.81)\n",
      "bers ↔ Sp (similarity: 0.81)\n",
      "bers ↔ es (similarity: 0.81)\n",
      "bers ↔ verb (similarity: 0.83)\n",
      "etz ↔ ung (similarity: 0.87)\n",
      "etz ↔ Se (similarity: 0.80)\n",
      "etz ↔ it (similarity: 0.81)\n",
      "etz ↔ es (similarity: 0.81)\n",
      "etz ↔ en (similarity: 0.83)\n",
      "ung ↔ en (similarity: 0.82)\n",
      "ung ↔ ung (similarity: 0.90)\n",
      "Se ↔ it (similarity: 0.89)\n",
      "Se ↔ en (similarity: 0.84)\n",
      "Se ↔ Sp (similarity: 0.85)\n",
      "it ↔ en (similarity: 0.87)\n",
      "en ↔ en (similarity: 0.87)\n",
      ". ↔ 00 (similarity: 0.82)\n",
      "Sp ↔ es (similarity: 0.82)\n",
      "Sp ↔ en (similarity: 0.81)\n",
      "Sp ↔ Total (similarity: 0.81)\n",
      "Sp ↔ Bank (similarity: 0.84)\n",
      "es ↔ en (similarity: 0.87)\n",
      "en ↔ ung (similarity: 0.81)\n",
      "Total ↔ Bank (similarity: 0.81)\n",
      "Bank ↔ verb (similarity: 0.86)\n",
      "Bank ↔ ind (similarity: 0.81)\n",
      "Bank ↔ ung (similarity: 0.81)\n",
      "Bank ↔ bank (similarity: 0.83)\n",
      "Bank ↔ IB (similarity: 0.80)\n",
      "verb ↔ ind (similarity: 0.84)\n",
      "ind ↔ ung (similarity: 0.84)\n",
      "ung ↔ : (similarity: 0.84)\n",
      "ung ↔ au (similarity: 0.81)\n",
      ": ↔ , (similarity: 0.84)\n",
      "A ↔ arg (similarity: 0.86)\n",
      "A ↔ au (similarity: 0.86)\n",
      "A ↔ ische (similarity: 0.83)\n",
      "A ↔ A (similarity: 0.95)\n",
      "A ↔ ar (similarity: 0.82)\n",
      "A ↔ au (similarity: 0.81)\n",
      "arg ↔ au (similarity: 0.88)\n",
      "arg ↔ ische (similarity: 0.85)\n",
      "arg ↔ onal (similarity: 0.84)\n",
      "arg ↔ bank (similarity: 0.81)\n",
      "arg ↔ A (similarity: 0.82)\n",
      "arg ↔ ar (similarity: 0.87)\n",
      "arg ↔ au (similarity: 0.84)\n",
      "arg ↔ AN (similarity: 0.81)\n",
      "au ↔ ische (similarity: 0.88)\n",
      "au ↔ onal (similarity: 0.84)\n",
      "au ↔ bank (similarity: 0.83)\n",
      "au ↔ A (similarity: 0.83)\n",
      "au ↔ ar (similarity: 0.82)\n",
      "au ↔ au (similarity: 0.92)\n",
      "ische ↔ Kant (similarity: 0.82)\n",
      "ische ↔ onal (similarity: 0.82)\n",
      "ische ↔ bank (similarity: 0.86)\n",
      "ische ↔ A (similarity: 0.80)\n",
      "ische ↔ au (similarity: 0.86)\n",
      "ische ↔ , (similarity: 0.80)\n",
      "ische ↔ AN (similarity: 0.82)\n",
      "ische ↔ CH (similarity: 0.82)\n",
      "ische ↔ 26 (similarity: 0.83)\n",
      "Kant ↔ onal (similarity: 0.84)\n",
      "Kant ↔ bank (similarity: 0.85)\n",
      "onal ↔ bank (similarity: 0.84)\n",
      "bank ↔ au (similarity: 0.81)\n",
      "A ↔ ar (similarity: 0.87)\n",
      "A ↔ au (similarity: 0.85)\n",
      "A ↔ IB (similarity: 0.82)\n",
      "A ↔ AN (similarity: 0.80)\n",
      "ar ↔ au (similarity: 0.87)\n",
      "ar ↔ IB (similarity: 0.81)\n",
      "ar ↔ AN (similarity: 0.83)\n",
      "au ↔ IB (similarity: 0.81)\n",
      "au ↔ AN (similarity: 0.84)\n",
      ", ↔ AN (similarity: 0.81)\n",
      "IB ↔ AN (similarity: 0.83)\n",
      "IB ↔ CH (similarity: 0.88)\n",
      "AN ↔ CH (similarity: 0.81)\n",
      "AN ↔ 26 (similarity: 0.85)\n",
      "CH ↔ 00 (similarity: 0.83)\n",
      "26 ↔ 76 (similarity: 0.86)\n",
      "26 ↔ 6 (similarity: 0.82)\n",
      "00 ↔ 101 (similarity: 0.85)\n",
      "76 ↔ 6 (similarity: 0.90)\n",
      "76 ↔ 84 (similarity: 0.82)\n",
      "101 ↔ 11 (similarity: 0.89)\n",
      "6 ↔ 84 (similarity: 0.82)\n",
      "11 ↔ 4 (similarity: 0.83)\n",
      "84 ↔ 349 (similarity: 0.84)\n",
      "4 ↔ 349 (similarity: 0.80)\n",
      "Z ↔ ahl (similarity: 0.83)\n",
      "ahl ↔ bar (similarity: 0.84)\n",
      ". ↔ . (similarity: 0.95)\n",
      "Shiva ↔ Sie (similarity: 0.80)\n",
      "Sie ↔ gen (similarity: 0.84)\n",
      "Sie ↔ sie (similarity: 0.83)\n",
      "gen ↔ en (similarity: 0.81)\n",
      "gen ↔ gen (similarity: 0.91)\n",
      "F ↔ Ã© (similarity: 0.84)\n",
      "F ↔ h (similarity: 0.85)\n",
      "F ↔ ren (similarity: 0.81)\n",
      "F ↔ we (similarity: 0.81)\n",
      "F ↔ g (similarity: 0.80)\n",
      "F ↔ Tele (similarity: 0.82)\n",
      "F ↔ f (similarity: 0.87)\n",
      "F ↔ E (similarity: 0.83)\n",
      "Ã© ↔ h (similarity: 0.83)\n",
      "Ã© ↔ ren (similarity: 0.85)\n",
      "Ã© ↔ f (similarity: 0.83)\n",
      "Ã© ↔ on (similarity: 0.80)\n",
      "h ↔ ren (similarity: 0.86)\n",
      "h ↔ we (similarity: 0.86)\n",
      "h ↔ g (similarity: 0.84)\n",
      "h ↔ f (similarity: 0.85)\n",
      "h ↔ on (similarity: 0.81)\n",
      "ren ↔ we (similarity: 0.87)\n",
      "ren ↔ g (similarity: 0.82)\n",
      "ren ↔ on (similarity: 0.82)\n",
      "we ↔ g (similarity: 0.88)\n",
      "we ↔ f (similarity: 0.80)\n",
      "we ↔ on (similarity: 0.81)\n",
      "g ↔ f (similarity: 0.80)\n",
      "g ↔ on (similarity: 0.82)\n",
      "g ↔ : (similarity: 0.81)\n",
      "6 ↔ 5 (similarity: 0.82)\n",
      "6 ↔ 56 (similarity: 0.80)\n",
      "5 ↔ 11 (similarity: 0.82)\n",
      "5 ↔ 70 (similarity: 0.80)\n",
      "400 ↔ 70 (similarity: 0.84)\n",
      "400 ↔ 40 (similarity: 0.83)\n",
      "en ↔ on (similarity: 0.82)\n",
      "en ↔ gen (similarity: 0.82)\n",
      "Tele ↔ on (similarity: 0.81)\n",
      "Tele ↔ Mobile (similarity: 0.81)\n",
      "Tele ↔ E (similarity: 0.83)\n",
      "Tele ↔ Mail (similarity: 0.80)\n",
      "f ↔ on (similarity: 0.85)\n",
      "f ↔ - (similarity: 0.81)\n",
      "on ↔ : (similarity: 0.81)\n",
      "on ↔ : (similarity: 0.81)\n",
      "on ↔ - (similarity: 0.83)\n",
      "on ↔ : (similarity: 0.80)\n",
      ": ↔ : (similarity: 0.99)\n",
      ": ↔ - (similarity: 0.81)\n",
      ": ↔ : (similarity: 0.96)\n",
      "0 ↔ 11 (similarity: 0.83)\n",
      "0 ↔ 0 (similarity: 0.99)\n",
      "0 ↔ 79 (similarity: 0.80)\n",
      "0 ↔ 20 (similarity: 0.81)\n",
      "56 ↔ 70 (similarity: 0.82)\n",
      "56 ↔ 79 (similarity: 0.90)\n",
      "222 ↔ 11 (similarity: 0.81)\n",
      "222 ↔ 70 (similarity: 0.81)\n",
      "222 ↔ 79 (similarity: 0.80)\n",
      "222 ↔ 257 (similarity: 0.94)\n",
      "222 ↔ 20 (similarity: 0.85)\n",
      "11 ↔ 70 (similarity: 0.84)\n",
      "11 ↔ 0 (similarity: 0.84)\n",
      "11 ↔ 257 (similarity: 0.83)\n",
      "11 ↔ 20 (similarity: 0.86)\n",
      "11 ↔ 40 (similarity: 0.83)\n",
      "70 ↔ 0 (similarity: 0.80)\n",
      "70 ↔ 79 (similarity: 0.85)\n",
      "70 ↔ 257 (similarity: 0.83)\n",
      "70 ↔ 20 (similarity: 0.87)\n",
      "70 ↔ 40 (similarity: 0.90)\n",
      "Mobile ↔ E (similarity: 0.81)\n",
      "Mobile ↔ Mail (similarity: 0.83)\n",
      ": ↔ - (similarity: 0.82)\n",
      ": ↔ : (similarity: 0.97)\n",
      "0 ↔ 79 (similarity: 0.81)\n",
      "0 ↔ 257 (similarity: 0.80)\n",
      "0 ↔ 20 (similarity: 0.83)\n",
      "0 ↔ 40 (similarity: 0.81)\n",
      "79 ↔ 257 (similarity: 0.83)\n",
      "257 ↔ 20 (similarity: 0.87)\n",
      "257 ↔ 40 (similarity: 0.82)\n",
      "20 ↔ 40 (similarity: 0.92)\n",
      "E ↔ - (similarity: 0.83)\n",
      "E ↔ Mail (similarity: 0.84)\n",
      "- ↔ Mail (similarity: 0.80)\n",
      "- ↔ : (similarity: 0.83)\n",
      "- ↔ . (similarity: 0.83)\n",
      ": ↔ . (similarity: 0.82)\n",
      "sh ↔ iva (similarity: 0.82)\n",
      "sh ↔ sie (similarity: 0.80)\n",
      "sh ↔ ch (similarity: 0.82)\n",
      "iva ↔ sie (similarity: 0.82)\n",
      "iva ↔ gen (similarity: 0.83)\n",
      "@ ↔ . (similarity: 0.82)\n",
      "sie ↔ gen (similarity: 0.84)\n",
      "sie ↔ ch (similarity: 0.81)\n",
      "gen ↔ ch (similarity: 0.84)\n",
      ". ↔ ch (similarity: 0.81)\n",
      "Fr ↔ Fr (similarity: 0.96)\n",
      "Fr ↔ Fr (similarity: 0.96)\n",
      ". ↔ . (similarity: 0.90)\n",
      ". ↔ . (similarity: 0.96)\n",
      ". ↔ . (similarity: 0.88)\n",
      ". ↔ . (similarity: 0.96)\n",
      ". ↔ . (similarity: 0.87)\n",
      "180 ↔ 00 (similarity: 0.80)\n",
      "180 ↔ 180 (similarity: 0.96)\n",
      ". ↔ 00 (similarity: 0.83)\n",
      ". ↔ . (similarity: 0.87)\n",
      ". ↔ . (similarity: 0.95)\n",
      ". ↔ 00 (similarity: 0.81)\n",
      ". ↔ . (similarity: 0.87)\n",
      ". ↔ . (similarity: 0.97)\n",
      ". ↔ 00 (similarity: 0.80)\n",
      "00 ↔ 00 (similarity: 0.81)\n",
      "00 ↔ 00 (similarity: 0.96)\n",
      "00 ↔ 00 (similarity: 0.96)\n",
      "Fr ↔ Fr (similarity: 0.99)\n",
      ". ↔ . (similarity: 0.91)\n",
      ". ↔ . (similarity: 0.99)\n",
      ". ↔ . (similarity: 0.88)\n",
      "00 ↔ 00 (similarity: 0.85)\n",
      "00 ↔ 180 (similarity: 0.84)\n",
      "00 ↔ 00 (similarity: 0.83)\n",
      ". ↔ 00 (similarity: 0.82)\n",
      ". ↔ . (similarity: 0.91)\n",
      ". ↔ . (similarity: 0.97)\n",
      ". ↔ 00 (similarity: 0.80)\n",
      "00 ↔ . (similarity: 0.82)\n",
      "00 ↔ 00 (similarity: 0.98)\n",
      ". ↔ . (similarity: 0.89)\n",
      ". ↔ 00 (similarity: 0.83)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_semantic_groups(token_info):\n",
    "    \"\"\"\n",
    "    Group tokens based on their embedding similarity and spatial proximity\n",
    "    \"\"\"\n",
    "    # Stack all embeddings\n",
    "    embeddings = torch.stack([info['embedding'] for info in token_info])\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = F.cosine_similarity(\n",
    "        embeddings.unsqueeze(1), \n",
    "        embeddings.unsqueeze(0), \n",
    "        dim=2\n",
    "    )\n",
    "    \n",
    "    # Create feature matrix combining embeddings and spatial info\n",
    "    features = []\n",
    "    for info in token_info:\n",
    "        # Normalize spatial coordinates to be on similar scale as embeddings\n",
    "        x_center = (info['box'][0] + info['box'][2]) / 2 / 1000  # Normalize by page width\n",
    "        y_center = (info['box'][1] + info['box'][3]) / 2 / 1000  # Normalize by page height\n",
    "        features.append([x_center, y_center])\n",
    "    \n",
    "    # Use DBSCAN for clustering\n",
    "    clustering = DBSCAN(eps=0.03, min_samples=2).fit(features)\n",
    "    \n",
    "    # Group tokens by cluster\n",
    "    groups = defaultdict(list)\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label != -1:  # -1 represents noise in DBSCAN\n",
    "            groups[label].append(token_info[idx])\n",
    "    \n",
    "    return groups, similarity_matrix\n",
    "\n",
    "def analyze_document_semantics(image_path, processor, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Analyze document using semantic grouping\n",
    "    \"\"\"\n",
    "    # Get token information\n",
    "    token_info = process_document_tokens(image_path, processor, model, device)\n",
    "    \n",
    "    # Get semantic groups\n",
    "    groups, similarity_matrix = calculate_semantic_groups(token_info)\n",
    "    \n",
    "    print(f\"\\nAnalyzing document: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Analyze each semantic group\n",
    "    print(\"\\nSemantic Groups Found:\")\n",
    "    for group_id, tokens in groups.items():\n",
    "        # Get average position for the group\n",
    "        avg_x = np.mean([t['box'][0] for t in tokens])\n",
    "        avg_y = np.mean([t['box'][1] for t in tokens])\n",
    "        \n",
    "        # Reconstruct text from tokens\n",
    "        text = ' '.join(t['token'] for t in tokens)\n",
    "        \n",
    "        # Calculate average embedding norm for the group\n",
    "        avg_norm = np.mean([t['embedding_norm'] for t in tokens])\n",
    "        \n",
    "        print(f\"\\nGroup {group_id}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Position: ({avg_x:.0f}, {avg_y:.0f})\")\n",
    "        print(f\"Importance (avg norm): {avg_norm:.2f}\")\n",
    "        \n",
    "        # Look for specific patterns in the group\n",
    "        has_numbers = any(any(c.isdigit() for c in t['token']) for t in tokens)\n",
    "        has_keywords = any(t['token'].lower() in ['total', 'summe', 'betrag', 'chf', 'fr'] for t in tokens)\n",
    "        \n",
    "        if has_numbers and has_keywords:\n",
    "            print(\"→ Potential amount field\")\n",
    "        elif avg_y < 100:  # Near top of page\n",
    "            print(\"→ Potential header information\")\n",
    "        elif avg_x < 200 and len(text) > 20:  # Left side, long text\n",
    "            print(\"→ Potential address block\")\n",
    "    \n",
    "    # Find highly similar token pairs\n",
    "    print(\"\\nStrongly Related Terms:\")\n",
    "    n_tokens = len(token_info)\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(i+1, n_tokens):\n",
    "            if similarity_matrix[i,j] > 0.8:  # High similarity threshold\n",
    "                print(f\"{token_info[i]['token']} ↔ {token_info[j]['token']}\" \n",
    "                      f\" (similarity: {similarity_matrix[i,j]:.2f})\")\n",
    "    \n",
    "    return groups, similarity_matrix, token_info\n",
    "\n",
    "# Try the semantic analysis\n",
    "documents = list_demo_documents()\n",
    "if documents:\n",
    "    image_path = os.path.join(DEMO_PATH, documents[0])\n",
    "    groups, similarity_matrix, token_info = analyze_document_semantics(\n",
    "        image_path, processor, model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available documents:\n",
      "0: 15014330_Shiva_Siegen_320000220000492023_1.png\n",
      "1: 15031152_Topmech_320000220010442023_1.png\n",
      "2: 50001213_KSU_A-Technik_320000220006912023_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaclavpechtor/Code/home_projects/document-processing-demo/env-layoutlm/lib/python3.11/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Structure Analysis:\n",
      "==================================================\n",
      "\n",
      "AMOUNT\n",
      "------------------------------\n",
      "Confidence: 0.75\n",
      "Alignment: left\n",
      "\n",
      "Content Groups:\n",
      "\n",
      "Group 1 (avg similarity: 1.00):\n",
      "Tokens: Total\n",
      "\n",
      "Group 2 (avg similarity: 0.81):\n",
      "Tokens: Sp\n",
      "\n",
      "Group 3 (avg similarity: 0.81):\n",
      "Tokens: Bank\n",
      "\n",
      "Group 4 (avg similarity: 0.78):\n",
      "Tokens: U\n",
      "\n",
      "Group 5 (avg similarity: 0.75):\n",
      "Tokens: en\n",
      "\n",
      "Group 6 (avg similarity: 0.75):\n",
      "Tokens: Land\n",
      "\n",
      "Group 7 (avg similarity: 0.73):\n",
      "Tokens: es\n",
      "\n",
      "Group 8 (avg similarity: 0.73):\n",
      "Tokens: :\n",
      "\n",
      "Group 9 (avg similarity: 0.73):\n",
      "Tokens: Le\n",
      "\n",
      "Group 10 (avg similarity: 0.73):\n",
      "Tokens: bers Se etz ung\n",
      "\n",
      "Group 11 (avg similarity: 0.72):\n",
      "Tokens: ung\n",
      "\n",
      "Group 12 (avg similarity: 0.72):\n",
      "Tokens: An\n",
      "\n",
      "Group 13 (avg similarity: 0.71):\n",
      "Tokens: 00\n",
      "\n",
      "Group 14 (avg similarity: 0.71):\n",
      "Tokens: :\n",
      "\n",
      "Group 15 (avg similarity: 0.71):\n",
      "Tokens: 2\n",
      "\n",
      "Group 16 (avg similarity: 0.70):\n",
      "Tokens: au\n",
      "\n",
      "Group 17 (avg similarity: 0.70):\n",
      "Tokens: Iran\n"
     ]
    }
   ],
   "source": [
    "def analyze_semantic_relationships(token_info):\n",
    "    \"\"\"Analyze semantic relationships between tokens using embeddings\"\"\"\n",
    "    # Stack all embeddings\n",
    "    embeddings = torch.stack([info['embedding'] for info in token_info])\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = F.cosine_similarity(\n",
    "        embeddings.unsqueeze(1), \n",
    "        embeddings.unsqueeze(0), \n",
    "        dim=2\n",
    "    )\n",
    "    \n",
    "    # Define semantic anchor points (important concepts in invoices)\n",
    "    anchors = {\n",
    "        'amount': ['total', 'summe', 'chf', 'fr.', 'betrag'],\n",
    "        'date': ['datum', 'date', 'vom', 'per'],\n",
    "        'vendor': ['firma', 'company', 'von:', 'from:', 'absender'],\n",
    "        'address': ['strasse', 'street', 'weg', 'plz', 'ort']\n",
    "    }\n",
    "    \n",
    "    # Find tokens most similar to each concept\n",
    "    semantic_groups = {}\n",
    "    for concept, keywords in anchors.items():\n",
    "        # Find tokens containing keywords\n",
    "        keyword_indices = []\n",
    "        for idx, info in enumerate(token_info):\n",
    "            if any(keyword in info['token'].lower() for keyword in keywords):\n",
    "                keyword_indices.append(idx)\n",
    "        \n",
    "        if keyword_indices:\n",
    "            # Calculate average similarity to keyword tokens\n",
    "            concept_similarities = similarity_matrix[:, keyword_indices].mean(dim=1)\n",
    "            \n",
    "            # Find related tokens (high similarity)\n",
    "            related_indices = torch.where(concept_similarities > 0.7)[0]\n",
    "            \n",
    "            # Group tokens with their similarity scores\n",
    "            semantic_groups[concept] = [\n",
    "                {\n",
    "                    'token': token_info[idx]['token'],\n",
    "                    'position': token_info[idx]['box'],\n",
    "                    'similarity': concept_similarities[idx].item(),\n",
    "                    'embedding_norm': token_info[idx]['embedding_norm']\n",
    "                }\n",
    "                for idx in related_indices\n",
    "            ]\n",
    "    \n",
    "    return semantic_groups\n",
    "\n",
    "def analyze_spatial_relationships(semantic_groups):\n",
    "    \"\"\"Analyze spatial relationships between semantic groups\"\"\"\n",
    "    spatial_patterns = {}\n",
    "    \n",
    "    for concept, tokens in semantic_groups.items():\n",
    "        # Calculate average position for the group\n",
    "        positions = np.array([token['position'] for token in tokens])\n",
    "        avg_position = positions.mean(axis=0)\n",
    "        \n",
    "        # Calculate spatial spread\n",
    "        position_std = positions.std(axis=0)\n",
    "        \n",
    "        # Analyze alignment\n",
    "        x_coords = positions[:, 0]\n",
    "        alignments = {\n",
    "            'left': np.abs(x_coords - x_coords.min()).mean(),\n",
    "            'right': np.abs(x_coords - x_coords.max()).mean()\n",
    "        }\n",
    "        \n",
    "        spatial_patterns[concept] = {\n",
    "            'average_position': avg_position,\n",
    "            'spread': position_std,\n",
    "            'alignment': 'left' if alignments['left'] < alignments['right'] else 'right'\n",
    "        }\n",
    "    \n",
    "    return spatial_patterns\n",
    "\n",
    "def extract_document_structure(image_path, processor, model, device='cpu'):\n",
    "    \"\"\"Extract structured information from document using semantic understanding\"\"\"\n",
    "    # Get token information\n",
    "    token_info = process_document_tokens(image_path, processor, model, device)\n",
    "    \n",
    "    # Analyze semantic relationships\n",
    "    semantic_groups = analyze_semantic_relationships(token_info)\n",
    "    \n",
    "    # Analyze spatial patterns\n",
    "    spatial_patterns = analyze_spatial_relationships(semantic_groups)\n",
    "    \n",
    "    # Combine semantic and spatial information\n",
    "    document_structure = {}\n",
    "    \n",
    "    for concept, tokens in semantic_groups.items():\n",
    "        spatial_info = spatial_patterns[concept]\n",
    "        \n",
    "        # Sort tokens by similarity and position\n",
    "        sorted_tokens = sorted(tokens, key=lambda x: (-x['similarity'], x['position'][1]))\n",
    "        \n",
    "        # Group nearby tokens\n",
    "        grouped_tokens = []\n",
    "        current_group = []\n",
    "        last_y = None\n",
    "        \n",
    "        for token in sorted_tokens:\n",
    "            current_y = token['position'][1]\n",
    "            \n",
    "            if last_y is None or abs(current_y - last_y) < 20:  # Threshold for vertical proximity\n",
    "                current_group.append(token)\n",
    "            else:\n",
    "                if current_group:\n",
    "                    grouped_tokens.append(current_group)\n",
    "                current_group = [token]\n",
    "            last_y = current_y\n",
    "        \n",
    "        if current_group:\n",
    "            grouped_tokens.append(current_group)\n",
    "        \n",
    "        # Store structured information\n",
    "        document_structure[concept] = {\n",
    "            'groups': grouped_tokens,\n",
    "            'spatial_pattern': spatial_info,\n",
    "            'confidence': np.mean([t['similarity'] for t in tokens])\n",
    "        }\n",
    "    \n",
    "    return document_structure\n",
    "\n",
    "def print_document_analysis(document_structure):\n",
    "    \"\"\"Print structured analysis of document\"\"\"\n",
    "    print(\"\\nDocument Structure Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for concept, info in document_structure.items():\n",
    "        print(f\"\\n{concept.upper()}\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Confidence: {info['confidence']:.2f}\")\n",
    "        print(f\"Alignment: {info['spatial_pattern']['alignment']}\")\n",
    "        \n",
    "        print(\"\\nContent Groups:\")\n",
    "        for idx, group in enumerate(info['groups']):\n",
    "            tokens = [t['token'] for t in group]\n",
    "            similarities = [t['similarity'] for t in group]\n",
    "            print(f\"\\nGroup {idx+1} (avg similarity: {np.mean(similarities):.2f}):\")\n",
    "            print(f\"Tokens: {' '.join(tokens)}\")\n",
    "\n",
    "# Try the enhanced analysis\n",
    "documents = list_demo_documents()\n",
    "if documents:\n",
    "    image_path = os.path.join(DEMO_PATH, documents[0])\n",
    "    document_structure = extract_document_structure(image_path, processor, model)\n",
    "    print_document_analysis(document_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
