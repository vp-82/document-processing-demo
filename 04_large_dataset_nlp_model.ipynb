{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966c7aaf-4d1a-401b-a25b-2bac0bb40d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: sudo apt-get update\n",
      "Command output: Hit:1 https://deb.nodesource.com/node_16.x focal InRelease\n",
      "Hit:2 http://ppa.launchpad.net/alex-p/tesseract-ocr/ubuntu focal InRelease\n",
      "Hit:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2681 kB]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1047 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3169 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2366 kB]\n",
      "Fetched 9598 kB in 7s (1429 kB/s)\n",
      "Reading package lists...\n",
      "\n",
      "Running command: sudo apt-get install software-properties-common -y\n",
      "Command output: Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "software-properties-common is already the newest version (0.99.9.11).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 139 not upgraded.\n",
      "\n",
      "Running command: sudo add-apt-repository ppa:alex-p/tesseract-ocr -y\n",
      "Command output: Hit:1 https://deb.nodesource.com/node_16.x focal InRelease\n",
      "Hit:2 http://ppa.launchpad.net/alex-p/tesseract-ocr/ubuntu focal InRelease\n",
      "Hit:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Fetched 108 kB in 37s (2963 B/s)\n",
      "Reading package lists...\n",
      "\n",
      "Running command: sudo apt-get update\n",
      "Command output: Hit:1 https://deb.nodesource.com/node_16.x focal InRelease\n",
      "Hit:2 http://ppa.launchpad.net/alex-p/tesseract-ocr/ubuntu focal InRelease\n",
      "Hit:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Fetched 108 kB in 6s (19.0 kB/s)\n",
      "Reading package lists...\n",
      "\n",
      "Running command: sudo apt-get install tesseract-ocr -y\n",
      "Command output: Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "tesseract-ocr is already the newest version (4.1.3-1ppa1~focal1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 139 not upgraded.\n",
      "\n",
      "Installing pytesseract...\n",
      "Requirement already satisfied: pytesseract in /usr/local/lib/python3.9/dist-packages (0.3.10)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (9.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mpytesseract installed successfully.\n",
      "Installing pillow...\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (9.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mpillow installed successfully.\n",
      "Installing transformers...\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.20.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mtransformers installed successfully.\n",
      "Installing scikit-learn...\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mscikit-learn installed successfully.\n",
      "Installing pandas...\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mpandas installed successfully.\n",
      "Installing datasets...\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mdatasets installed successfully.\n",
      "Installing wandb...\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.15.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.3.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mwandb installed successfully.\n",
      "Installing python-dotenv...\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mpython-dotenv installed successfully.\n",
      "Importing libraries...\n",
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Run the setup_04.py script to set up the necessary dependencies.\n",
    "!python setup_04.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84952d15-fb2b-4d48-bdea-ebb84efd1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "    import os\n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import pandas as pd\n",
    "    from datasets import load_dataset, DatasetDict\n",
    "    import wandb\n",
    "    from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c0d321-82bd-4687-bff2-89e7f2d1ba10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python script\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542c2351-af5f-470b-b2c7-f9e8891c8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpect\u001b[0m (\u001b[33mzhaw-sml-iwi-it_strategy_management\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a4ea2e-9431-4358-af27-58b3386ae8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration vaclavpechtor--rvl_cdip-small-200-853f638e95c0bf72\n",
      "Reusing dataset imagefolder (./dataset/rvl-cdip-small-200/hf_cache/vaclavpechtor___imagefolder/vaclavpechtor--rvl_cdip-small-200-853f638e95c0bf72/0.0.0/48efdc62d40223daee675ca093d163bcb6cb0b7d7f93eb25aebf5edca72dc597)\n",
      "Using custom data configuration vaclavpechtor--rvl_cdip-small-200-853f638e95c0bf72\n",
      "Reusing dataset imagefolder (./dataset/rvl-cdip-small-200/hf_cache/vaclavpechtor___imagefolder/vaclavpechtor--rvl_cdip-small-200-853f638e95c0bf72/0.0.0/48efdc62d40223daee675ca093d163bcb6cb0b7d7f93eb25aebf5edca72dc597)\n"
     ]
    }
   ],
   "source": [
    "# Loading train and validation datasets from Hugging Face Datasets\n",
    "train_dataset = load_dataset(\"vaclavpechtor/rvl_cdip-small-200\", split=\"train\", cache_dir=\"./dataset/rvl-cdip-small-200/hf_cache\")\n",
    "validation_dataset = load_dataset(\"vaclavpechtor/rvl_cdip-small-200\", split=\"validation\", cache_dir=\"./dataset/rvl-cdip-small-200/hf_cache\")\n",
    "# Combining both train and validation datasets into a single DatasetDict\n",
    "ocr_dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507baa7b-e634-4dc2-99c7-216831766efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from class labels to IDs and vice versa\n",
    "class_labels = ocr_dataset['train'].features['label'].names\n",
    "label_to_id = {label: i for i, label in enumerate(class_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0782f8e0-41ca-4f26-9e22-759fd9c5e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pickle for serializing and deserializing Python object structures\n",
    "import pickle\n",
    "# Function to save a dataset to a pickle file\n",
    "def save_dataset(dataset, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7444841-8aa0-409f-875b-f6e631e51144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a dataset from a pickle file\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb8f9c8-c809-43e6-aca8-44b289c0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform OCR on an image and return the extracted text\n",
    "def ocr_image(img):\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2cbb16-1995-444e-a358-aad7fb21f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from pkl...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Checking if the serialized dataset exists\n",
    "# If it does, load it. Otherwise, perform OCR on the images, map the labels to IDs, and then serialize the dataset.\n",
    "if os.path.exists('./dataset/rvl-cdip-small-200/ocr_dataset.pkl'):\n",
    "    print('Loading dataset from pkl...')\n",
    "    ocr_dataset = load_dataset('./dataset/rvl-cdip-small-200/ocr_dataset.pkl')\n",
    "else:\n",
    "    print('Running OCR...')\n",
    "    ocr_dataset = ocr_dataset.map(lambda x: {\"text\": ocr_image(x[\"image\"])})\n",
    "    ocr_dataset = ocr_dataset.map(lambda example: {'label': label_to_id[example['label']] if isinstance(example['label'], str) else example['label']})\n",
    "    save_dataset(ocr_dataset, './dataset/rvl-cdip-small-200/ocr_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e0052f-f775-459e-81e5-e3c25874da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label', 'text'],\n",
       "        num_rows: 2560\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label', 'text'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the dataset\n",
    "ocr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209d178c-d11f-4f45-afd2-26bc90736e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the dataset format to pandas for easier data manipulation\n",
    "ocr_dataset.set_format(type='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27303209-a034-4c11-b744-f8298475edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the training data as a pandas DataFrame\n",
    "df = ocr_dataset['train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc0ccec-6c29-4d98-8c99-dca488969491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\na\\n\\nCevetrom Phi\"eMonis,\\n\\n” Saratoe\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>eae Arizona\\nFebruary 20-21, 1984 oo\\n\\nSit 00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>ae\\nPRIVEE SITET\\neae beaters\\n\\n \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>a\\nholds backtar;\\nf ut lets Ue Sul.\\nmenthol ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>fl .\\na\\n\\n \\n\\n \\n\\nyou should know that many...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label  \\\n",
       "0  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "1  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "2  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "3  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "4  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "\n",
       "                                                text  \n",
       "0   \\n\\na\\n\\nCevetrom Phi\"eMonis,\\n\\n” Saratoe\\n\\...  \n",
       "1  eae Arizona\\nFebruary 20-21, 1984 oo\\n\\nSit 00...  \n",
       "2              ae\\nPRIVEE SITET\\neae beaters\\n\\n \\n\n",
       "  \n",
       "3  a\\nholds backtar;\\nf ut lets Ue Sul.\\nmenthol ...  \n",
       "4  fl .\\na\\n\\n \\n\\n \\n\\nyou should know that many...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing the first few rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7e84dd-dfb6-4463-83cc-116e5b4f4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column 'category_name' that maps the 'label' to the actual class name\n",
    "df['category_name'] = df['label'].map(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6576de9a-c135-4213-8803-e48fb97787c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\na\\n\\nCevetrom Phi\"eMonis,\\n\\n” Saratoe\\n\\...</td>\n",
       "      <td>advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>eae Arizona\\nFebruary 20-21, 1984 oo\\n\\nSit 00...</td>\n",
       "      <td>advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>ae\\nPRIVEE SITET\\neae beaters\\n\\n \\n</td>\n",
       "      <td>advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>a\\nholds backtar;\\nf ut lets Ue Sul.\\nmenthol ...</td>\n",
       "      <td>advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.TiffImagePlugin.TiffImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>fl .\\na\\n\\n \\n\\n \\n\\nyou should know that many...</td>\n",
       "      <td>advertisement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label  \\\n",
       "0  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "1  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "2  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "3  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "4  <PIL.TiffImagePlugin.TiffImageFile image mode=...      0   \n",
       "\n",
       "                                                text  category_name  \n",
       "0   \\n\\na\\n\\nCevetrom Phi\"eMonis,\\n\\n” Saratoe\\n\\...  advertisement  \n",
       "1  eae Arizona\\nFebruary 20-21, 1984 oo\\n\\nSit 00...  advertisement  \n",
       "2              ae\\nPRIVEE SITET\\neae beaters\\n\\n \\n\n",
       "  advertisement  \n",
       "3  a\\nholds backtar;\\nf ut lets Ue Sul.\\nmenthol ...  advertisement  \n",
       "4  fl .\\na\\n\\n \\n\\n \\n\\nyou should know that many...  advertisement  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dee9fe1-8e2d-494e-94af-411e13d607de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverting the dataset format to the default format\n",
    "ocr_dataset.set_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "050eab3e-5437-47fe-ac2c-fa12be4baf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c641d6f43b84083acde72671616692e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5377e565dae14120bb205da478f46f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f487c0ce65c43d9ac9dcfe475afc56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55edf5303d4f4731ac03364ebab657c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031dba2b8ef5407d9d2590ef0e7623cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Specifying the model checkpoint to use\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "# Initializing the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e44b2d8-ce9a-4b4d-bb7b-fb1503ec148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing accuracy_score from sklearn.metrics to compute the accuracy of our model\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Defining a function to compute the accuracy of the model's predictions\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b64afa7f-1166-4b01-a0e5-ee58e222fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Parameter 'function'=<function tokenize_function at 0x7f480792b8b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c5078dde3e4b629872324db95b1da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a65a12383432aa32c30ac44fb8750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: image, text. If image, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2560\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpect\u001b[0m (\u001b[33mzhaw-sml-iwi-it_strategy_management\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230515_135649-pyj2au69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhaw-sml-iwi-it_strategy_management/document-processing-demo/runs/pyj2au69' target=\"_blank\">doc_demo_20230515_135645</a></strong> to <a href='https://wandb.ai/zhaw-sml-iwi-it_strategy_management/document-processing-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhaw-sml-iwi-it_strategy_management/document-processing-demo' target=\"_blank\">https://wandb.ai/zhaw-sml-iwi-it_strategy_management/document-processing-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhaw-sml-iwi-it_strategy_management/document-processing-demo/runs/pyj2au69' target=\"_blank\">https://wandb.ai/zhaw-sml-iwi-it_strategy_management/document-processing-demo/runs/pyj2au69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 06:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.576834</td>\n",
       "      <td>0.564063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.867200</td>\n",
       "      <td>1.280182</td>\n",
       "      <td>0.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.867200</td>\n",
       "      <td>1.190104</td>\n",
       "      <td>0.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>1.125183</td>\n",
       "      <td>0.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.564900</td>\n",
       "      <td>1.140371</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: image, text. If image, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/checkpoint-320\n",
      "Configuration saved in output/checkpoint-320/config.json\n",
      "Model weights saved in output/checkpoint-320/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: image, text. If image, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/checkpoint-640\n",
      "Configuration saved in output/checkpoint-640/config.json\n",
      "Model weights saved in output/checkpoint-640/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: image, text. If image, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/checkpoint-960\n",
      "Configuration saved in output/checkpoint-960/config.json\n",
      "Model weights saved in output/checkpoint-960/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: image, text. If image, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/checkpoint-1280\n",
      "Configuration saved in output/checkpoint-1280/config.json\n",
      "Model weights saved in output/checkpoint-1280/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: image, text. If image, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output/checkpoint-1600\n",
      "Configuration saved in output/checkpoint-1600/config.json\n",
      "Model weights saved in output/checkpoint-1600/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output/checkpoint-1280 (score: 0.6796875).\n",
      "Saving model checkpoint to /tmp/tmp3bkczxbj\n",
      "Configuration saved in /tmp/tmp3bkczxbj/config.json\n",
      "Model weights saved in /tmp/tmp3bkczxbj/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1600, training_loss=1.0885858297348023, metrics={'train_runtime': 361.547, 'train_samples_per_second': 35.403, 'train_steps_per_second': 4.425, 'total_flos': 842061211238400.0, 'train_loss': 1.0885858297348023, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# Specifying the model checkpoint\n",
    "model_checkpoint = \"bert-base-uncased\"  # or whatever model you're using\n",
    "# Initializing the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "num_labels = 16  # adjust this according to your task\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Function to tokenize our text data\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    # Convert tensors to numpy arrays\n",
    "    return {key: value.numpy() for key, value in output.items()}\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenized_dataset = ocr_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Naming the run for tracking in Weights & Biases\n",
    "run_name = \"doc_demo_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Defining the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"wandb\",  # enables reporting to W&B\n",
    "    run_name=run_name,  # name of the W&B run\n",
    "    logging_dir='./wandb',  # directory where the run files will be stored\n",
    ")\n",
    "\n",
    "# Initializing the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,  # Add this line\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56b4219-fefb-44c8-8b47-6a60a653b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./models/distilbert-base-uncased_model/config.json\n",
      "Model weights saved in ./models/distilbert-base-uncased_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/distilbert-base-uncased_tokenizer/tokenizer_config.json\n",
      "Special tokens file saved in ./models/distilbert-base-uncased_tokenizer/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/distilbert-base-uncased_tokenizer/tokenizer_config.json',\n",
       " './models/distilbert-base-uncased_tokenizer/special_tokens_map.json',\n",
       " './models/distilbert-base-uncased_tokenizer/vocab.txt',\n",
       " './models/distilbert-base-uncased_tokenizer/added_tokens.json',\n",
       " './models/distilbert-base-uncased_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/distilbert-base-uncased_model\")\n",
    "tokenizer.save_pretrained(\"./models/distilbert-base-uncased_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d73fd0-af8e-4c24-b534-82df58759898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
