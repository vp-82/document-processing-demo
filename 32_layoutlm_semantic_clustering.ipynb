{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_demo_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m groups, similarity_matrix, token_info\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Try the semantic analysis\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mlist_demo_documents\u001b[49m()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents:\n\u001b[1;32m     95\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DEMO_PATH, documents[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_demo_documents' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_semantic_groups(token_info):\n",
    "    \"\"\"\n",
    "    Group tokens based on their embedding similarity and spatial proximity\n",
    "    \"\"\"\n",
    "    # Stack all embeddings\n",
    "    embeddings = torch.stack([info['embedding'] for info in token_info])\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = F.cosine_similarity(\n",
    "        embeddings.unsqueeze(1), \n",
    "        embeddings.unsqueeze(0), \n",
    "        dim=2\n",
    "    )\n",
    "    \n",
    "    # Create feature matrix combining embeddings and spatial info\n",
    "    features = []\n",
    "    for info in token_info:\n",
    "        # Normalize spatial coordinates to be on similar scale as embeddings\n",
    "        x_center = (info['box'][0] + info['box'][2]) / 2 / 1000  # Normalize by page width\n",
    "        y_center = (info['box'][1] + info['box'][3]) / 2 / 1000  # Normalize by page height\n",
    "        features.append([x_center, y_center])\n",
    "    \n",
    "    # Use DBSCAN for clustering\n",
    "    clustering = DBSCAN(eps=0.03, min_samples=2).fit(features)\n",
    "    \n",
    "    # Group tokens by cluster\n",
    "    groups = defaultdict(list)\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label != -1:  # -1 represents noise in DBSCAN\n",
    "            groups[label].append(token_info[idx])\n",
    "    \n",
    "    return groups, similarity_matrix\n",
    "\n",
    "def analyze_document_semantics(image_path, processor, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Analyze document using semantic grouping\n",
    "    \"\"\"\n",
    "    # Get token information\n",
    "    token_info = process_document_tokens(image_path, processor, model, device)\n",
    "    \n",
    "    # Get semantic groups\n",
    "    groups, similarity_matrix = calculate_semantic_groups(token_info)\n",
    "    \n",
    "    print(f\"\\nAnalyzing document: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Analyze each semantic group\n",
    "    print(\"\\nSemantic Groups Found:\")\n",
    "    for group_id, tokens in groups.items():\n",
    "        # Get average position for the group\n",
    "        avg_x = np.mean([t['box'][0] for t in tokens])\n",
    "        avg_y = np.mean([t['box'][1] for t in tokens])\n",
    "        \n",
    "        # Reconstruct text from tokens\n",
    "        text = ' '.join(t['token'] for t in tokens)\n",
    "        \n",
    "        # Calculate average embedding norm for the group\n",
    "        avg_norm = np.mean([t['embedding_norm'] for t in tokens])\n",
    "        \n",
    "        print(f\"\\nGroup {group_id}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Position: ({avg_x:.0f}, {avg_y:.0f})\")\n",
    "        print(f\"Importance (avg norm): {avg_norm:.2f}\")\n",
    "        \n",
    "        # Look for specific patterns in the group\n",
    "        has_numbers = any(any(c.isdigit() for c in t['token']) for t in tokens)\n",
    "        has_keywords = any(t['token'].lower() in ['total', 'summe', 'betrag', 'chf', 'fr'] for t in tokens)\n",
    "        \n",
    "        if has_numbers and has_keywords:\n",
    "            print(\"→ Potential amount field\")\n",
    "        elif avg_y < 100:  # Near top of page\n",
    "            print(\"→ Potential header information\")\n",
    "        elif avg_x < 200 and len(text) > 20:  # Left side, long text\n",
    "            print(\"→ Potential address block\")\n",
    "    \n",
    "    # Find highly similar token pairs\n",
    "    print(\"\\nStrongly Related Terms:\")\n",
    "    n_tokens = len(token_info)\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(i+1, n_tokens):\n",
    "            if similarity_matrix[i,j] > 0.8:  # High similarity threshold\n",
    "                print(f\"{token_info[i]['token']} ↔ {token_info[j]['token']}\" \n",
    "                      f\" (similarity: {similarity_matrix[i,j]:.2f})\")\n",
    "    \n",
    "    return groups, similarity_matrix, token_info\n",
    "\n",
    "# Try the semantic analysis\n",
    "documents = list_demo_documents()\n",
    "if documents:\n",
    "    image_path = os.path.join(DEMO_PATH, documents[0])\n",
    "    groups, similarity_matrix, token_info = analyze_document_semantics(\n",
    "        image_path, processor, model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
